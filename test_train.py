import torch
import unittest
import argparse
from unittest import TestCase
from visual import plot_historical_loss
from train import Trainer_blp, Trainer_blg, Trainer_blp_node_recon, Trainer_blp_structured_node_recon

class TestTrain(TestCase): 
    def test_1_cuda_available(self):
        self.assertEqual(torch.cuda.is_available(), True)

    # def test_2_trainer_blp(self): 
    #     parser = argparse.ArgumentParser()
    #     parser.add_argument('--hidden_channels', type = int, default = 64, help = 'number of hidden channels (encoder)')
    #     parser.add_argument('--out_channels', type = int, default = 128, help = 'number of output channels (encoder)')
    #     parser.add_argument('--lr', type = float, default = 1e-3, help = 'learning rate')
    #     parser.add_argument('--weight_decay', type = float, default = 5e-4, help = 'adam weight decay')
    #     parser.add_argument('--epoch', type = int, default = 1000, help = 'number of training epochs')
    #     parser.add_argument('--period', type = int, default = 100, help = 'save model every n epochs')
    #     parser.add_argument('--data',  type = str, default = 'sim_dataset_test', help = 'training data folder')
    #     parser.add_argument('--seed',  type = int, default = 2025, help = 'training reproductivity')    
    #     parser.add_argument('--save',  type = str, help = 'checkpoint save path')
    #     parser.add_argument('--model', type = str, default = 'blp-test', help = 'model name')      
    #     parser.add_argument('--resume', type = str, help = 'load model from checkpoints')  
    #     parser.add_argument('--swap', type = float, default = 0.0, help = 'probability of feature swaping') 
    #     parser.add_argument('--log', type = str, help = 'log file name') 
    #     args = parser.parse_args()
    #     trainer = Trainer_blp()
    #     trainer.train(args)

    # def test_3_trainer_blg(self): 
    #     parser = argparse.ArgumentParser()
    #     parser.add_argument('--hidden_channels', type = int, default = 16, help = 'number of hidden channels (encoder)')
    #     parser.add_argument('--u_dim', type = int, default = 64, help = 'number of hidden channels (encoder)')
    #     parser.add_argument('--v_dim', type = int, default = 64, help = 'number of hidden channels (encoder)')
    #     parser.add_argument('--out_channels', type = int, default = 32, help = 'number of output channels (encoder)')
    #     parser.add_argument('--lr', type = float, default = 1e-3, help = 'learning rate')
    #     parser.add_argument('--weight_decay', type = float, default = 5e-4, help = 'adam weight decay')
    #     parser.add_argument('--epoch', type = int, default = 1000, help = 'number of training epochs')
    #     parser.add_argument('--period', type = int, default = 100, help = 'save model every n epochs')
    #     parser.add_argument('--data',  type = str, default = 'sim_dataset_test', help = 'training data folder')
    #     parser.add_argument('--seed',  type = int, default = 2025, help = 'training reproductivity')    
    #     parser.add_argument('--model', type = str, default = 'blg-test-16-16', help = 'model name')
    #     parser.add_argument('--swap', type = float, default = 0.0, help = 'probability of feature swaping')  
    #     parser.add_argument('--save',  type = str, help = 'checkpoint save path')   
    #     parser.add_argument('--resume', type = str, help = 'load model from checkpoints')  
    #     parser.add_argument('--log', type = str, help = 'log file name') 
    #     args = parser.parse_args()
    #     trainer = Trainer_blg()
    #     trainer.train(args)

    # def test_4_trainer_blp_n(self): 
    #     parser = argparse.ArgumentParser()
    #     parser.add_argument('--hidden_channels', type = int, default = 64, help = 'number of hidden channels (encoder)')
    #     parser.add_argument('--out_channels', type = int, default = 128, help = 'number of output channels (encoder)')
    #     parser.add_argument('--lr', type = float, default = 1e-3, help = 'learning rate')
    #     parser.add_argument('--weight_decay', type = float, default = 5e-4, help = 'adam weight decay')
    #     parser.add_argument('--epoch', type = int, default = 200, help = 'number of training epochs')
    #     parser.add_argument('--period', type = int, default = 100, help = 'save model every n epochs')
    #     parser.add_argument('--data',  type = str, default = 'sim_dataset_test', help = 'training data folder')
    #     parser.add_argument('--seed',  type = int, default = 2025, help = 'training reproductivity')    
    #     parser.add_argument('--model', type = str, default = 'blp-n-test', help = 'model name')        
    #     parser.add_argument('--swap', type = float, default = 0.0, help = 'probability of feature swaping')
    #     parser.add_argument('--resume', type = str, help = 'load model from checkpoints')
    #     parser.add_argument('--save',  type = str, help = 'checkpoint save path') 
    #     parser.add_argument('--log', type = str, help = 'log file name') 
    #     args = parser.parse_args()
    #     # trainer = Trainer_blg()
    #     trainer = Trainer_blp_node_recon()
    #     l_train, l_test, l_val, l_node = trainer.train(args)
    #     plot_historical_loss(x=range(args.epoch), y_list=[l_train, l_test, l_val], label_list=['train', 'test', 'validation'], savefig="edge_loss.png")
    #     plot_historical_loss(x=range(args.epoch), y_list=[l_node], label_list=['node loss'], savefig="node_loss.png")

    def test_5_trainer_blp_b(self): 
        parser = argparse.ArgumentParser()
        parser.add_argument('--hidden_channels', type = int, default = 64, help = 'number of hidden channels (encoder)')
        parser.add_argument('--out_channels', type = int, default = 128, help = 'number of output channels (encoder)')
        parser.add_argument('--lr', type = float, default = 1e-3, help = 'learning rate')
        parser.add_argument('--weight_decay', type = float, default = 5e-4, help = 'adam weight decay')
        parser.add_argument('--epoch', type = int, default = 200, help = 'number of training epochs')
        parser.add_argument('--period', type = int, default = 100, help = 'save model every n epochs')
        parser.add_argument('--data',  type = str, default = 'sim_dataset_v2', help = 'training data folder')
        parser.add_argument('--seed',  type = int, default = 2025, help = 'training reproductivity')    
        parser.add_argument('--model', type = str, default = 'blp-b-test', help = 'model name')        
        # parser.add_argument('--swap', type = float, default = 0.0, help = 'probability of feature swaping')
        # parser.add_argument('--gamma', type = float, default = 1e-2, help = 'loss weight')
        parser.add_argument('--resume', type = str, help = 'load model from checkpoints')
        parser.add_argument('--save',  type = str, help = 'checkpoint save path') 
        parser.add_argument('--log', type = str, help = 'log file name') 
        args = parser.parse_args()
        trainer = Trainer_blp_structured_node_recon()
        loss_train, loss_test = trainer.train(args)
        plot_historical_loss(x=range(args.epoch), y_list=[loss_train, loss_test], label_list=['train', 'test'], savefig="loss.png")

unittest.main()